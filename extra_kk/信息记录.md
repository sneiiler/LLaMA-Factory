2025年6月26日 08：53
(llama) root@ji-jupyter-9079258979990331392-master-0:~/coder/LLaMA-Factory# CUDA_VISIBLE_DEVICES=0,1,2,3 llamafactory-cli train examples/train_full/train_qwen3_14B_full_sft.
yaml 
INFO 06-26 00:48:27 [__init__.py:244] Automatically detected platform cuda.
[INFO|2025-06-26 00:48:30] llamafactory.cli:143 >> Initializing 4 distributed tasks at: ji-jupyter-9079258979990331392-master-0.ji-jupyter-9079258979990331392:7788
W0626 00:48:31.609000 236959 site-packages/torch/distributed/run.py:766] 
W0626 00:48:31.609000 236959 site-packages/torch/distributed/run.py:766] *****************************************
W0626 00:48:31.609000 236959 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0626 00:48:31.609000 236959 site-packages/torch/distributed/run.py:766] *****************************************
[INFO|2025-06-26 00:48:38] llamafactory.hparams.parser:406 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-06-26 00:48:38] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,535 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,535 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,535 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,535 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,535 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,535 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,535 >> loading file chat_template.jinja
[INFO|2025-06-26 00:48:38] llamafactory.hparams.parser:406 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-06-26 00:48:38] llamafactory.hparams.parser:406 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2299] 2025-06-26 00:48:38,893 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:696] 2025-06-26 00:48:38,905 >> loading configuration file /mnt/kaifeng/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:770] 2025-06-26 00:48:38,911 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,926 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,926 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,927 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,927 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,927 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,927 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-26 00:48:38,927 >> loading file chat_template.jinja
[rank1]:[W626 00:48:39.681936806 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[INFO|tokenization_utils_base.py:2299] 2025-06-26 00:48:39,294 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-06-26 00:48:39] llamafactory.data.loader:143 >> Loading dataset identity.json...
[rank2]:[W626 00:48:39.213684790 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank3]:[W626 00:48:39.226827947 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
Converting format of dataset (num_proc=16): 100%|████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:00<00:00, 452.91 examples/s]
[INFO|2025-06-26 00:48:40] llamafactory.data.loader:143 >> Loading dataset distilled_training_data_v20250618_sharegpt_format_v1.json...
Converting format of dataset (num_proc=16): 100%|█████████████████████████████████████████████████████████████████████████████████| 232/232 [00:00<00:00, 1152.58 examples/s]
[rank0]:[W626 00:48:41.033205980 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
ji-jupyter-9079258979990331392-master-0:237036:237036 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
ji-jupyter-9079258979990331392-master-0:237036:237036 [0] NCCL INFO Bootstrap: Using eth0:172.20.3.37<0>
ji-jupyter-9079258979990331392-master-0:237036:237036 [0] NCCL INFO cudaDriverVersion 12030
ji-jupyter-9079258979990331392-master-0:237036:237036 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
ji-jupyter-9079258979990331392-master-0:237036:237036 [0] NCCL INFO Comm config Blocking set to 1
ji-jupyter-9079258979990331392-master-0:237037:237037 [1] NCCL INFO cudaDriverVersion 12030
ji-jupyter-9079258979990331392-master-0:237037:237037 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
ji-jupyter-9079258979990331392-master-0:237037:237037 [1] NCCL INFO Bootstrap: Using eth0:172.20.3.37<0>
ji-jupyter-9079258979990331392-master-0:237037:237037 [1] NCCL INFO NCCL version 2.26.2+cuda12.2
ji-jupyter-9079258979990331392-master-0:237039:237039 [3] NCCL INFO cudaDriverVersion 12030
ji-jupyter-9079258979990331392-master-0:237039:237039 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
ji-jupyter-9079258979990331392-master-0:237039:237039 [3] NCCL INFO Bootstrap: Using eth0:172.20.3.37<0>
ji-jupyter-9079258979990331392-master-0:237039:237039 [3] NCCL INFO NCCL version 2.26.2+cuda12.2
ji-jupyter-9079258979990331392-master-0:237037:237037 [1] NCCL INFO Comm config Blocking set to 1
ji-jupyter-9079258979990331392-master-0:237038:237038 [2] NCCL INFO cudaDriverVersion 12030
ji-jupyter-9079258979990331392-master-0:237038:237038 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
ji-jupyter-9079258979990331392-master-0:237038:237038 [2] NCCL INFO Bootstrap: Using eth0:172.20.3.37<0>
ji-jupyter-9079258979990331392-master-0:237038:237038 [2] NCCL INFO NCCL version 2.26.2+cuda12.2
ji-jupyter-9079258979990331392-master-0:237039:237039 [3] NCCL INFO Comm config Blocking set to 1
ji-jupyter-9079258979990331392-master-0:237038:237038 [2] NCCL INFO Comm config Blocking set to 1
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Failed to open libibverbs.so[.1]
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO NET/Socket : Using [0]eth0:172.20.3.37<0>
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Using network Socket
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO ncclCommInitRankConfig comm 0xf9c3100 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 49000 commId 0xe13553aadfcc9955 - Init START
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Failed to open libibverbs.so[.1]
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO NET/Socket : Using [0]eth0:172.20.3.37<0>
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Using network Socket
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Failed to open libibverbs.so[.1]
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO NET/Socket : Using [0]eth0:172.20.3.37<0>
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Using network Socket
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Failed to open libibverbs.so[.1]
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO NET/Socket : Using [0]eth0:172.20.3.37<0>
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Using network Socket
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO ncclCommInitRankConfig comm 0x14e75db0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 8f000 commId 0xe13553aadfcc9955 - Init START
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO ncclCommInitRankConfig comm 0x14d16120 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 4d000 commId 0xe13553aadfcc9955 - Init START
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO RAS client listening socket at ::1<28028>
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO ncclCommInitRankConfig comm 0x157520e0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 8a000 commId 0xe13553aadfcc9955 - Init START
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO RAS client listening socket at ::1<28028>
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO RAS client listening socket at ::1<28028>
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO RAS client listening socket at ::1<28028>
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Bootstrap timings total 0.167517 (create 0.000028, send 0.000078, recv 0.121356, ring 0.045750, delay 0.000000)
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Bootstrap timings total 0.048700 (create 0.000036, send 0.000135, recv 0.000184, ring 0.000305, delay 0.000001)
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Bootstrap timings total 0.038948 (create 0.000028, send 0.000120, recv 0.000179, ring 0.000750, delay 0.000000)
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Bootstrap timings total 0.046331 (create 0.000028, send 0.000111, recv 0.007487, ring 0.000112, delay 0.000001)
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff,00000000
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO NVLS multicast support is not available on dev 2
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO NVLS multicast support is not available on dev 1
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff,00000000
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO NVLS multicast support is not available on dev 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO NVLS multicast support is not available on dev 0
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO comm 0x14e75db0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO comm 0x157520e0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO comm 0xf9c3100 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO comm 0x14d16120 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO P2P Chunksize set to 524288
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO P2P Chunksize set to 524288
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 00/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 01/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 02/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 03/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO P2P Chunksize set to 524288
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 04/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 05/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 06/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 07/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 08/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 09/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 10/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 11/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 12/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 13/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 14/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 15/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 16/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 17/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 18/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 19/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 20/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 21/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 22/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 23/24 : 0 1 2 3
ji-jupyter-9079258979990331392-master-0:237039:237299 [3] NCCL INFO [Proxy Service] Device 3 CPU core 96
ji-jupyter-9079258979990331392-master-0:237038:237298 [2] NCCL INFO [Proxy Service] Device 2 CPU core 36
ji-jupyter-9079258979990331392-master-0:237039:237300 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 101
ji-jupyter-9079258979990331392-master-0:237038:237301 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 96
ji-jupyter-9079258979990331392-master-0:237037:237302 [1] NCCL INFO [Proxy Service] Device 1 CPU core 7
ji-jupyter-9079258979990331392-master-0:237037:237303 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 8
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO P2P Chunksize set to 524288
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 1 directMode 0
ji-jupyter-9079258979990331392-master-0:237036:237304 [0] NCCL INFO [Proxy Service] Device 0 CPU core 68
ji-jupyter-9079258979990331392-master-0:237036:237305 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 04/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 13/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 14/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 16/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 17/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 19/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC/read
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Connected all trees
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Connected all trees
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Connected all trees
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Connected all trees
ji-jupyter-9079258979990331392-master-0:237039:237306 [3] NCCL INFO [Proxy Progress] Device 3 CPU core 35
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
ji-jupyter-9079258979990331392-master-0:237038:237307 [2] NCCL INFO [Proxy Progress] Device 2 CPU core 101
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
ji-jupyter-9079258979990331392-master-0:237037:237309 [1] NCCL INFO [Proxy Progress] Device 1 CPU core 67
ji-jupyter-9079258979990331392-master-0:237036:237308 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO CC Off, workFifoBytes 1048576
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO ncclCommInitRankConfig comm 0x14e75db0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 8f000 commId 0xe13553aadfcc9955 - Init COMPLETE
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO ncclCommInitRankConfig comm 0x157520e0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 8a000 commId 0xe13553aadfcc9955 - Init COMPLETE
ji-jupyter-9079258979990331392-master-0:237039:237274 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 0.80 (kernels 0.14, alloc 0.12, bootstrap 0.05, allgathers 0.00, topo 0.21, graphs 0.00, connections 0.26, rest 0.02)
ji-jupyter-9079258979990331392-master-0:237038:237275 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 0.79 (kernels 0.15, alloc 0.12, bootstrap 0.04, allgathers 0.00, topo 0.20, graphs 0.00, connections 0.27, rest 0.01)
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO ncclCommInitRankConfig comm 0x14d16120 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 4d000 commId 0xe13553aadfcc9955 - Init COMPLETE
ji-jupyter-9079258979990331392-master-0:237037:237273 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 0.80 (kernels 0.14, alloc 0.12, bootstrap 0.05, allgathers 0.00, topo 0.20, graphs 0.00, connections 0.28, rest 0.00)
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO ncclCommInitRankConfig comm 0xf9c3100 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 49000 commId 0xe13553aadfcc9955 - Init COMPLETE
ji-jupyter-9079258979990331392-master-0:237036:237272 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 0.81 (kernels 0.14, alloc 0.02, bootstrap 0.17, allgathers 0.00, topo 0.21, graphs 0.00, connections 0.28, rest 0.01)
Running tokenizer on dataset (num_proc=16): 100%|██████████████████████████████████████████████████████████████████████████████████| 327/327 [00:02<00:00, 150.92 examples/s]
training example:
input_ids:
[151644, 872, 198, 6023, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 9707, 0, 358, 1079, 24522, 42213, 21886, 2435, 36694, 20286, 4903, 11, 458, 15235, 17847, 7881, 553, 5616, 15815, 315, 11487, 11791, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151645, 198]
inputs:
<|im_start|>user
hi<|im_end|>
<|im_start|>assistant
<think>

</think>

Hello! I am Constellation Dynamic Clustering Large Model, an AI assistant developed by China Academy of Space Technology. How can I assist you today?<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 271, 151668, 271, 9707, 0, 358, 1079, 24522, 42213, 21886, 2435, 36694, 20286, 4903, 11, 458, 15235, 17847, 7881, 553, 5616, 15815, 315, 11487, 11791, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151645, 198]
labels:
<think>

</think>

Hello! I am Constellation Dynamic Clustering Large Model, an AI assistant developed by China Academy of Space Technology. How can I assist you today?<|im_end|>

[INFO|configuration_utils.py:696] 2025-06-26 00:48:44,697 >> loading configuration file /mnt/kaifeng/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:770] 2025-06-26 00:48:44,699 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-06-26 00:48:44] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|modeling_utils.py:1148] 2025-06-26 00:48:44,907 >> loading weights file /mnt/kaifeng/models/Qwen3-14B/model.safetensors.index.json
[INFO|modeling_utils.py:2241] 2025-06-26 00:48:44,911 >> Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-06-26 00:48:44,915 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.32it/s]
[INFO|modeling_utils.py:5131] 2025-06-26 00:48:51,041 >> All model checkpoint weights were used when initializing Qwen3ForCausalLM.

[INFO|modeling_utils.py:5139] 2025-06-26 00:48:51,041 >> All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at /mnt/kaifeng/models/Qwen3-14B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-06-26 00:48:51,047 >> loading configuration file /mnt/kaifeng/models/Qwen3-14B/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-26 00:48:51,048 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[INFO|2025-06-26 00:48:51] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-06-26 00:48:51] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-26 00:48:51] llamafactory.model.adapter:143 >> Pure bf16 / BAdam detected, remaining trainable params in half precision.
[INFO|2025-06-26 00:48:51] llamafactory.model.adapter:143 >> Fine-tuning method: Full
[INFO|2025-06-26 00:48:51] llamafactory.model.loader:143 >> trainable params: 14,768,307,200 || all params: 14,768,307,200 || trainable%: 100.0000
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:756] 2025-06-26 00:48:51,078 >> Using auto half precision backend
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.32it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.28it/s]
[INFO|trainer.py:2409] 2025-06-26 00:48:52,483 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-06-26 00:48:52,483 >>   Num examples = 327
[INFO|trainer.py:2411] 2025-06-26 00:48:52,483 >>   Num Epochs = 3
[INFO|trainer.py:2412] 2025-06-26 00:48:52,483 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2415] 2025-06-26 00:48:52,483 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:2416] 2025-06-26 00:48:52,483 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2417] 2025-06-26 00:48:52,483 >>   Total optimization steps = 123
[INFO|trainer.py:2418] 2025-06-26 00:48:52,484 >>   Number of trainable parameters = 14,768,307,200
  0%|                                                                                                                                                | 0/123 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):
[rank0]:   File "/home/coder/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank0]:     launch()
[rank0]:   File "/home/coder/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank0]:     run_exp()
[rank0]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank0]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2240, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 3791, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/accelerator.py", line 2473, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/function.py", line 307, in apply
[rank0]:     return user_fn(self, *args)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 303, in backward
[rank0]:     outputs = ctx.run_function(*detached_inputs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 300, in forward
[rank0]:     hidden_states = self.mlp(hidden_states)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 90, in forward
[rank0]:     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 6.19 MiB is free. Process 4035663 has 79.13 GiB memory in use. Of the allocated memory 77.42 GiB is allocated by PyTorch, and 428.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/coder/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank1]:     launch()
[rank1]:   File "/home/coder/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank1]:     run_exp()
[rank1]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank1]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2240, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 3791, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/accelerator.py", line 2473, in backward
[rank1]:     loss.backward(**kwargs)
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/function.py", line 307, in apply
[rank1]:     return user_fn(self, *args)
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 320, in backward
[rank1]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 1 has a total capacity of 79.15 GiB of which 2.19 MiB is free. Process 4035665 has 79.13 GiB memory in use. Of the allocated memory 77.23 GiB is allocated by PyTorch, and 484.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/coder/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank2]:     launch()
[rank2]:   File "/home/coder/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank2]:     run_exp()
[rank2]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank2]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2240, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 3791, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/accelerator.py", line 2473, in backward
[rank2]:     loss.backward(**kwargs)
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/function.py", line 307, in apply
[rank2]:     return user_fn(self, *args)
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 320, in backward
[rank2]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 2 has a total capacity of 79.15 GiB of which 2.19 MiB is free. Process 4035666 has 79.13 GiB memory in use. Of the allocated memory 77.23 GiB is allocated by PyTorch, and 484.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|                                                                                                                                                | 0/123 [00:02<?, ?it/s]
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/coder/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank3]:     launch()
[rank3]:   File "/home/coder/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank3]:     run_exp()
[rank3]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank3]:   File "/home/coder/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2240, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 3791, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/accelerator.py", line 2473, in backward
[rank3]:     loss.backward(**kwargs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/function.py", line 307, in apply
[rank3]:     return user_fn(self, *args)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 303, in backward
[rank3]:     outputs = ctx.run_function(*detached_inputs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 300, in forward
[rank3]:     hidden_states = self.mlp(hidden_states)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 90, in forward
[rank3]:     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank3]:     return F.linear(input, self.weight, self.bias)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 3 has a total capacity of 79.15 GiB of which 6.19 MiB is free. Process 4035668 has 79.13 GiB memory in use. Of the allocated memory 77.42 GiB is allocated by PyTorch, and 428.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W626 00:48:54.622049595 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ji-jupyter-9079258979990331392-master-0:237036:237566 [0] NCCL INFO misc/socket.cc:64 -> 3
ji-jupyter-9079258979990331392-master-0:237036:237566 [0] NCCL INFO misc/socket.cc:80 -> 3
ji-jupyter-9079258979990331392-master-0:237036:237566 [0] NCCL INFO misc/socket.cc:829 -> 3
ji-jupyter-9079258979990331392-master-0:237036:237304 [0] NCCL INFO misc/socket.cc:881 -> 3
ji-jupyter-9079258979990331392-master-0:237037:237568 [1] NCCL INFO misc/socket.cc:64 -> 3
ji-jupyter-9079258979990331392-master-0:237037:237568 [1] NCCL INFO misc/socket.cc:80 -> 3
ji-jupyter-9079258979990331392-master-0:237037:237568 [1] NCCL INFO misc/socket.cc:829 -> 3
ji-jupyter-9079258979990331392-master-0:237037:237302 [1] NCCL INFO misc/socket.cc:881 -> 3
ji-jupyter-9079258979990331392-master-0:237038:237570 [2] NCCL INFO misc/socket.cc:64 -> 3
ji-jupyter-9079258979990331392-master-0:237038:237570 [2] NCCL INFO misc/socket.cc:80 -> 3
ji-jupyter-9079258979990331392-master-0:237038:237570 [2] NCCL INFO misc/socket.cc:829 -> 3
ji-jupyter-9079258979990331392-master-0:237038:237298 [2] NCCL INFO misc/socket.cc:881 -> 3
ji-jupyter-9079258979990331392-master-0:237039:237572 [3] NCCL INFO misc/socket.cc:64 -> 3
ji-jupyter-9079258979990331392-master-0:237039:237572 [3] NCCL INFO misc/socket.cc:80 -> 3
ji-jupyter-9079258979990331392-master-0:237039:237572 [3] NCCL INFO misc/socket.cc:829 -> 3
ji-jupyter-9079258979990331392-master-0:237039:237299 [3] NCCL INFO misc/socket.cc:881 -> 3
ji-jupyter-9079258979990331392-master-0:237036:237566 [0] NCCL INFO comm 0xf9c3100 rank 0 nranks 4 cudaDev 0 busId 49000 - Abort COMPLETE
ji-jupyter-9079258979990331392-master-0:237037:237568 [1] NCCL INFO comm 0x14d16120 rank 1 nranks 4 cudaDev 1 busId 4d000 - Abort COMPLETE
ji-jupyter-9079258979990331392-master-0:237038:237570 [2] NCCL INFO comm 0x157520e0 rank 2 nranks 4 cudaDev 2 busId 8a000 - Abort COMPLETE
ji-jupyter-9079258979990331392-master-0:237039:237572 [3] NCCL INFO comm 0x14e75db0 rank 3 nranks 4 cudaDev 3 busId 8f000 - Abort COMPLETE
W0626 00:48:56.585000 236959 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 237037 closing signal SIGTERM
W0626 00:48:56.586000 236959 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 237038 closing signal SIGTERM
W0626 00:48:56.586000 236959 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 237039 closing signal SIGTERM
E0626 00:48:57.617000 236959 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 237036) of binary: /opt/miniconda3/envs/llama/bin/python3.10
Traceback (most recent call last):
  File "/opt/miniconda3/envs/llama/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/coder/LLaMA-Factory/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-26_00:48:56
  host      : ji-jupyter-9079258979990331392-master-0.ji-jupyter-9079258979990331392.nhss-job.svc.cluster.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 237036)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/opt/miniconda3/envs/llama/bin/llamafactory-cli", line 8, in <module>
    sys.exit(main())
  File "/home/coder/LLaMA-Factory/src/llamafactory/cli.py", line 130, in main
    process = subprocess.run(
  File "/opt/miniconda3/envs/llama/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', 'ji-jupyter-9079258979990331392-master-0.ji-jupyter-9079258979990331392', '--master_port', '7788', '/home/coder/LLaMA-Factory/src/llamafactory/launcher.py', 'examples/train_full/train_qwen3_14B_full_sft.yaml']' returned non-zero exit status 1.